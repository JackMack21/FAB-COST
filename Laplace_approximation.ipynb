{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Laplace approximation taken from (ref...)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "def logistic_prob(X, w):\n",
    "\n",
    "    # set a truncation exponent.\n",
    "    trunc = 8.  # exp(8)/(1+exp(8)) = 0.9997 which is close enough to 1 as to not matter in most cases.\n",
    "\n",
    "    # calculate argument of logit\n",
    "    z = np.dot(X, w)\n",
    "\n",
    "    # truncate to avoid numerical over/underflow\n",
    "    z = np.clip(z, -trunc, trunc)\n",
    "\n",
    "    # calculate logitstic probability\n",
    "    pr = np.exp(z)\n",
    "    pr = pr / (1. + pr)\n",
    "\n",
    "    return pr\n",
    "\n",
    "\n",
    "def H_log_posterior(w, wprior, H, y, X, weights=None):\n",
    "    \"\"\"Returns Hessian (either full or diagonal) of the negative log posterior probability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : array-like, shape (p, )\n",
    "        parameter vector at which the Hessian is to be evaluated\n",
    "    wprior : array-like, shape (p, )\n",
    "        array of prior means on the parameters to be fit\n",
    "    H : array-like, shape (p, p) or (p, )\n",
    "        array of log prior Hessian (inverse covariance of prior distribution of parameters)\n",
    "    y : array-like, shape (N, )\n",
    "        array of binary ({0,1} responses)\n",
    "    X : array-like, shape (N, p)\n",
    "        array of features\n",
    "    weights : array-like, shape (N, )\n",
    "        array of data point weights. Should be within [0,1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H_log_post : array-like, shape like `H`\n",
    "                Hessian of negative log posterior\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Chapter 8 of Murphy, K. 'Machine Learning a Probabilistic Perspective', MIT Press (2012)\n",
    "    Chapter 4 of Bishop, C. 'Pattern Recognition and Machine Learning', Springer (2006)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # fill in weights if need be\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(np.atleast_1d(y)), )\n",
    "    if len(np.atleast_1d(weights)) != len(np.atleast_1d(y)):\n",
    "        raise ValueError(' weight vector must be same length as response vector')\n",
    "\n",
    "    # calculate log posterior Hessian\n",
    "\n",
    "    mu = logistic_prob(X, w)\n",
    "\n",
    "    S = mu * (1. - mu) * weights\n",
    "\n",
    "    if len(H.shape) == 2:\n",
    "        H_log_post = np.dot(X.T, X * S[:, np.newaxis]) + H\n",
    "    elif len(H.shape) == 1:\n",
    "        H_log_post = np.diag(np.dot(X.T, X * S[:, np.newaxis])) + H\n",
    "    else:\n",
    "        raise ValueError('Incompatible Hessian')\n",
    "\n",
    "    return H_log_post\n",
    "\n",
    "\n",
    "def g_log_posterior(w, wprior, H, y, X, weights=None):\n",
    "    \"\"\"Returns gradient of the negative log posterior probability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : array-like, shape (p, )\n",
    "        parameter vector at which the gradient is to be evaluated\n",
    "    wprior : array-like, shape (p, )\n",
    "        array of prior means on the parameters to be fit\n",
    "    H : array-like, shape (p, p) or (p, )\n",
    "        array of prior Hessian (inverse covariance of prior distribution of parameters)\n",
    "    y : array-like, shape (N, )\n",
    "        array of binary ({0,1} responses)\n",
    "    X : array-like, shape (N, p)\n",
    "        array of features\n",
    "    weights : array-like, shape (N, )\n",
    "        array of data point weights. Should be within [0,1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad_log_post : array-like, shape (p, )\n",
    "                gradient of negative log posterior\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Chapter 8 of Murphy, K. 'Machine Learning a Probabilistic Perspective', MIT Press (2012)\n",
    "    Chapter 4 of Bishop, C. 'Pattern Recognition and Machine Learning', Springer (2006)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # fill in weights if need be\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(np.atleast_1d(y)), )\n",
    "    if len(np.atleast_1d(weights)) != len(np.atleast_1d(y)):\n",
    "        raise ValueError(' weight vector must be same length as response vector')\n",
    "\n",
    "    # calculate gradient\n",
    "\n",
    "    mu_ = logistic_prob(X, w)\n",
    "\n",
    "    if len(H.shape) == 2:\n",
    "        grad_log_post = np.dot(X.T, weights * (mu_ - y)) + np.dot(H, (w - wprior))\n",
    "    elif len(H.shape) == 1:\n",
    "        grad_log_post = np.dot(X.T, weights * (mu_ - y)) + H * (w - wprior)\n",
    "    else:\n",
    "        raise ValueError('Incompatible Hessian')\n",
    "\n",
    "    return grad_log_post\n",
    "\n",
    "def f_log_posterior(w, wprior, H, y, X, weights=None):\n",
    "    \"\"\"Returns negative log posterior probability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : array-like, shape (p, )\n",
    "        vector of parameters at which the negative log posterior is to be evaluated\n",
    "    wprior : array-like, shape (p, )\n",
    "        vector of prior means on the parameters to be fit\n",
    "    H : array-like, shape (p, p) or (p, )\n",
    "        Array of prior Hessian (inverse covariance of prior distribution of parameters)\n",
    "    y : array-like, shape (N, )\n",
    "        vector of binary ({0,1} responses)\n",
    "    X : array-like, shape (N, p)\n",
    "        array of features\n",
    "    weights : array-like, shape (N, )\n",
    "        vector of data point weights. Should be within [0,1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    neg_log_post : float\n",
    "                negative log posterior probability\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Chapter 8 of Murphy, K. 'Machine Learning a Probabilistic Perspective', MIT Press (2012)\n",
    "    Chapter 4 of Bishop, C. 'Pattern Recognition and Machine Learning', Springer (2006)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # fill in weights if need be\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(np.atleast_1d(y)), )\n",
    "    if len(np.atleast_1d(weights)) != len(np.atleast_1d(y)):\n",
    "        raise ValueError(' weight vector must be same length as response vector')\n",
    "\n",
    "    # calculate negative log posterior\n",
    "\n",
    "    eps = 1e-6  # this defined to ensure that we never take a log of zero\n",
    "\n",
    "    mu = logistic_prob(X, w)\n",
    "\n",
    "    if len(H.shape) == 2:\n",
    "        neg_log_post = (- (np.dot(y.T, weights * np.log(mu + eps))\n",
    "                           + np.dot((1. - y).T, weights * np.log(1. - mu + eps)))\n",
    "                        + 0.5 * np.dot((w - wprior).T, np.dot(H, (w - wprior))))\n",
    "    elif len(H.shape) == 1:\n",
    "        neg_log_post = (- (np.dot(y.T, weights * np.log(mu + eps))\n",
    "                           + np.dot((1. - y).T, weights * np.log(1. - mu + eps)))\n",
    "                        + 0.5 * np.dot((w - wprior).T, H * (w - wprior)))\n",
    "    else:\n",
    "        raise ValueError('Incompatible Hessian')\n",
    "\n",
    "    return float(neg_log_post)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_bayes_logistic(y, X, wprior, H, weights=None, solver='Newton-CG', bounds=None, maxiter=100):\n",
    "    \"\"\" Bayesian Logistic Regression Solver.  Assumes Laplace (Gaussian) Approximation\n",
    "    to the posterior of the fitted parameter vector. Uses scipy.optimize.minimize\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like, shape (N, )\n",
    "        array of binary {0,1} responses\n",
    "    X : array-like, shape (N, p)\n",
    "        array of features\n",
    "    wprior : array-like, shape (p, )\n",
    "        array of prior means on the parameters to be fit\n",
    "    H : array-like, shape (p, p) or (p, )\n",
    "        array of prior Hessian (inverse covariance of prior distribution of parameters)\n",
    "    weights : array-like, shape (N, )\n",
    "        array of data point weights. Should be within [0,1]\n",
    "    solver : string\n",
    "        scipy optimize solver used.  this should be either 'Newton-CG', 'BFGS' or 'L-BFGS-B'.\n",
    "        The default is Newton-CG.\n",
    "    bounds : iterable of length p\n",
    "        a length p list (or tuple) of tuples each of length 2.\n",
    "        This is only used if the solver is set to 'L-BFGS-B'. In that case, a tuple\n",
    "        (lower_bound, upper_bound), both floats, is defined for each parameter.  See the\n",
    "        scipy.optimize.minimize docs for further information.\n",
    "    maxiter : int\n",
    "        maximum number of iterations for scipy.optimize.minimize solver.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w_fit : array-like, shape (p, )\n",
    "        posterior parameters (MAP estimate)\n",
    "    H_fit : array-like, shape like `H`\n",
    "        posterior Hessian  (Hessian of negative log posterior evaluated at MAP parameters)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Chapter 8 of Murphy, K. 'Machine Learning a Probabilistic Perspective', MIT Press (2012)\n",
    "    Chapter 4 of Bishop, C. 'Pattern Recognition and Machine Learning', Springer (2006)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that dimensionality of inputs agrees\n",
    "\n",
    "    # check X\n",
    "    if len(X.shape) != 2:\n",
    "        raise ValueError('X must be a N*p matrix')\n",
    "    (nX, pX) = X.shape\n",
    "\n",
    "    # check y\n",
    "    if len(y.shape) > 1:\n",
    "        raise ValueError('y must be a vector of shape (p, )')\n",
    "    if len(np.atleast_1d(y)) != nX:\n",
    "        raise ValueError('y and X do not have the same number of rows')\n",
    "\n",
    "    # check wprior\n",
    "    if len(wprior.shape) > 1:\n",
    "        raise ValueError('prior should be a vector of shape (p, )')\n",
    "    if len(np.atleast_1d(wprior)) != pX:\n",
    "        raise ValueError('prior mean has incompatible length')\n",
    "\n",
    "    # check H\n",
    "    if len(H.shape) == 1:\n",
    "        if np.atleast_1d(H).shape[0] != pX:\n",
    "            raise ValueError('prior Hessian is diagonal but has incompatible length')\n",
    "    elif len(H.shape) == 2:\n",
    "        (h1,h2) = np.atleast_2d(H).shape\n",
    "        if h1 != h2:\n",
    "            raise ValueError('prior Hessian must either be a p*p square matrix or a vector or shape (p, ) ')\n",
    "        if h1 != pX:\n",
    "            raise ValueError('prior Hessian is square but has incompatible size')\n",
    "\n",
    "    # fill in weights if need be\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(np.atleast_1d(y)), )\n",
    "    if len(np.atleast_1d(weights)) != len(np.atleast_1d(y)):\n",
    "        raise ValueError(' weight vector must be same length as response vector')\n",
    "\n",
    "    # Do the regression\n",
    "\n",
    "    if solver == 'Newton-CG':\n",
    "\n",
    "        if len(H.shape) == 2:\n",
    "\n",
    "            ww = minimize(f_log_posterior, wprior, args=(wprior, H, y, X, weights), jac=g_log_posterior,\n",
    "                          hess=H_log_posterior, method='Newton-CG', options={'maxiter': maxiter})\n",
    "            w_fit = ww.x\n",
    "            H_fit = H_log_posterior(w_fit, wprior, H, y, X, weights)\n",
    "\n",
    "        elif len(H.shape) == 1:\n",
    "\n",
    "            ww = minimize(f_log_posterior, wprior, args=(wprior, H, y, X, weights), jac=g_log_posterior,\n",
    "                          hessp=HP_log_posterior, method='Newton-CG', options={'maxiter': maxiter})\n",
    "            w_fit = ww.x\n",
    "            H_fit = H_log_posterior(w_fit, wprior, H, y, X, weights)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(' You must either use the full Hessian or its diagonal as a vector')\n",
    "\n",
    "    elif solver == 'BFGS':\n",
    "        ww = minimize(f_log_posterior, wprior, args=(wprior, H, y, X, weights), jac=g_log_posterior_small,\n",
    "                      method='BFGS', options={'maxiter': maxiter})\n",
    "        w_fit = ww.x\n",
    "        H_fit = H_log_posterior(w_fit, wprior, H, y, X, weights)\n",
    "\n",
    "    elif solver == 'L-BFGS-B':\n",
    "        ww = minimize(f_log_posterior, wprior, args=(wprior, H, y, X, weights), jac=g_log_posterior_small,\n",
    "                      method='L-BFGS-B', bounds=bounds, options={'maxiter': maxiter})\n",
    "        w_fit = ww.x\n",
    "        H_fit = H_log_posterior(w_fit, wprior, H, y, X, weights)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Unknown solver specified: \"{0}\"'.format(solver))\n",
    "\n",
    "    global w_laplace\n",
    "    global H_laplace\n",
    "    w_laplace = w_fit\n",
    "    H_laplace = H_fit\n",
    "    \n",
    "    return w_laplace, H_laplace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
